{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c13aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.feature_extraction import stop_words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "import transformers\n",
    "from ipywidgets import FloatProgress\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1384f996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.9/site-packages (4.16.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib64/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib64/python3.9/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in ./.local/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in ./.local/lib/python3.9/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers) (4.62.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: sacremoses in ./.local/lib/python3.9/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3.9/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.9/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3.9/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa33a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=os.getcwd()\n",
    "test_set=\"book30-listing-test.csv\"\n",
    "train_set=\"book30-listing-train.csv\"\n",
    "\n",
    "test=pd.read_csv(os.path.join(data_folder+\"/\"+test_set), sep=\",\", encoding='latin_1') #'unicode_escape'\n",
    "train=pd.read_csv(os.path.join(data_folder+\"/\"+train_set), sep=\",\", encoding='latin_1')\n",
    "#setting up columns' names\n",
    "test=test.set_axis([\"AMAZON INDEX (ASIN)\",\"FILENAME\",\"IMAGE URL\",\"TITLE\",\"AUTHOR\",\"CATEGORY ID\",\"CATEGORY\"], axis=1, inplace=False)\n",
    "train=train.set_axis([\"AMAZON INDEX (ASIN)\",\"FILENAME\",\"IMAGE URL\",\"TITLE\",\"AUTHOR\",\"CATEGORY ID\",\"CATEGORY\"], axis=1, inplace=False)\n",
    "\n",
    "#selecting title and category id\n",
    "test_set = test[['TITLE','CATEGORY ID']]\n",
    "train_set = train[['TITLE','CATEGORY ID']]\n",
    "#train_set2 = train_set.sample(frac=1).reset_index(drop=True) #shuffling for val data set\n",
    "#print(train_set)\n",
    "#print(train_set2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34a0785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45598.4 5699.8 5699.8\n",
      "LENGTH OF VALIDATION SET: 5701,       LENGTH OF TEST SET: 5699,        LENGTH OF TRAIN SET: 45598. \n"
     ]
    }
   ],
   "source": [
    "length = len(train_set) + len(test_set)\n",
    "##print(len(test_set))\n",
    "#print(length)\n",
    "print(0.8*length, 0.1* length, 0.1*length)\n",
    "\n",
    "train_s=train_set[:45598] #change to train_set2\n",
    "val_s=train_set[45598:] #change to train_set2\n",
    "\n",
    "print(f'LENGTH OF VALIDATION SET: {len(val_s)}, \\\n",
    "      LENGTH OF TEST SET: {len(test_set)},  \\\n",
    "      LENGTH OF TRAIN SET: {len(train_s)}. ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab810127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in test_set: 53\n",
      "Max number of words in train_set: 74\n",
      "Index =  4984\n",
      "['[', 'Antidiabetic', 'Agents:', 'Recent', 'Advances', 'in', 'Their', 'Molecular', 'and', 'Clinical', 'Pharmacology', '[', 'ANTIDIABETIC', 'AGENTS:', 'RECENT', 'ADVANCES', 'IN', 'THEIR', 'MOLECULAR', 'AND', 'CLINICAL', 'PHARMACOLOGY', 'BY', 'Testa', '-.', 'Meyer,', '-.', 'Meyer', '(', 'Author', ')', 'May-28-1996[', 'ANTIDIABETIC', 'AGENTS:', 'RECENT', 'ADVANCES', 'IN', 'THEIR', 'MOLECULAR', 'AND', 'CLINICAL', 'PHARMACOLOGY', '[', 'ANTIDIABETIC', 'AGENTS:', 'RECENT', 'ADVANCES', 'IN', 'THEIR', 'MOLECULAR', 'AND', 'CLINICAL', 'PHARMACOLOGY', 'BY', 'TESTA', '-.', 'MEYER,', '-.', 'MEYER', '(', 'AUTHOR', ')', 'MAY-28-1996', ']', 'By', 'Testa', '-.', 'Meyer,', '-.', 'Meyer', '(', 'Author', ')May-28-1996', 'Hardcover']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased',\n",
    "                                         do_lower_case=True)\n",
    "\n",
    "#checking for the longest title -> we want to set padding to max_length instead of 512\n",
    "length_of_the_messages_test = test_set['TITLE'].str.split(\"\\\\s+\")\n",
    "length_of_the_messages_train = train_set['TITLE'].str.split(\"\\\\s+\")\n",
    "\n",
    "print(f\"Max number of words in test_set: {length_of_the_messages_test.str.len().max()}\")\n",
    "print(f\"Max number of words in train_set: {length_of_the_messages_train.str.len().max()}\")\n",
    "print(\"Index = \", length_of_the_messages_train.str.len().idxmax())\n",
    "\n",
    "print(length_of_the_messages_train[4984])\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X):\n",
    "\n",
    "        self.labels = [label for label in X['CATEGORY ID']]  #list\n",
    "        self.texts = [tokenizer(title, truncation=True, padding='max_length', max_length = 74,  #512\n",
    "                    return_tensors=\"pt\") for title in X['TITLE']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded55150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased', num_labels=30) #bert-base-multilingual-cased\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 30) #30 since we have 30 categories\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89245d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True) #2\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=1) #2\n",
    "    \n",
    "    #os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    #DEVICE = torch.device('cuda:2')\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "    #        CUDA_LAUNCH_BLOCKING=1\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1fcdec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 45598/45598 [47:24<00:00, 16.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  2.568                 | Train Accuracy:  0.343                 | Val Loss:  2.238                 | Val Accuracy:  0.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 45598/45598 [48:15<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  1.695                 | Train Accuracy:  0.561                 | Val Loss:  1.755                 | Val Accuracy:  0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 45598/45598 [47:28<00:00, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  1.355                 | Train Accuracy:  0.647                 | Val Loss:  1.666                 | Val Accuracy:  0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 45598/45598 [47:43<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  1.083                 | Train Accuracy:  0.725                 | Val Loss:  1.611                 | Val Accuracy:  0.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 45598/45598 [48:11<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.821                 | Train Accuracy:  0.798                 | Val Loss:  1.633                 | Val Accuracy:  0.573\n"
     ]
    }
   ],
   "source": [
    "                  \n",
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6            \n",
    "train(model, train_s, val_s, LR, EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ff5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    \n",
    "evaluate(model, test_set)\n",
    "\n",
    "#learn_rate = [1e-6, 0.001, 0.01, 0.1, 0.2, 0.3]      \n",
    "#for LR in learn_rate:\n",
    "#    train(model, train_s, val_s, LR, EPOCHS)\n",
    "#    evaluate(model, test_set)\n",
    "\n",
    "#print('max length=74')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d91fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best_model.pth') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
