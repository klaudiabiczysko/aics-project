{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2cb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "import os, sys, time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.feature_extraction import stop_words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "import transformers\n",
    "from ipywidgets import FloatProgress\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "plt.ion()   # interactive mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91aa68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=os.getcwd()\n",
    "test_set=\"book30-listing-test.csv\"\n",
    "train_set=\"book30-listing-train.csv\"\n",
    "\n",
    "test=pd.read_csv(os.path.join(data_folder+\"/\"+test_set), sep=\",\", encoding='latin_1') #'unicode_escape'\n",
    "train=pd.read_csv(os.path.join(data_folder+\"/\"+train_set), sep=\",\", encoding='latin_1')\n",
    "#setting up columns' names\n",
    "test=test.set_axis([\"AMAZON INDEX (ASIN)\",\"FILENAME\",\"IMAGE URL\",\"TITLE\",\"AUTHOR\",\"CATEGORY ID\",\"CATEGORY\"], axis=1, inplace=False)\n",
    "train=train.set_axis([\"AMAZON INDEX (ASIN)\",\"FILENAME\",\"IMAGE URL\",\"TITLE\",\"AUTHOR\",\"CATEGORY ID\",\"CATEGORY\"], axis=1, inplace=False)\n",
    "\n",
    "#selecting title and category id\n",
    "test_set = test[['FILENAME','TITLE','CATEGORY ID', \"CATEGORY\"]]\n",
    "train_set = train[['FILENAME','TITLE','CATEGORY ID', \"CATEGORY\"]]\n",
    "#train_set2 = train_set.sample(frac=1).reset_index(drop=True) #shuffling for val data set\n",
    "#print(train_set)\n",
    "#print(train_set2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8ea3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45598.4 5699.8 5699.8\n",
      "LENGTH OF VALIDATION SET: 5701,       LENGTH OF TEST SET: 5699,        LENGTH OF TRAIN SET: 45598. \n"
     ]
    }
   ],
   "source": [
    "length = len(train_set) + len(test_set)\n",
    "##print(len(test_set))\n",
    "#print(length)\n",
    "print(0.8*length, 0.1* length, 0.1*length)\n",
    "\n",
    "train_s=train_set[:45598] #change to train_set2\n",
    "val_s=train_set[45598:] #change to train_set2\n",
    "\n",
    "print(f'LENGTH OF VALIDATION SET: {len(val_s)}, \\\n",
    "      LENGTH OF TEST SET: {len(test_set)},  \\\n",
    "      LENGTH OF TRAIN SET: {len(train_s)}. ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa4702d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in test_set: 53\n",
      "Max number of words in train_set: 74\n",
      "Index =  4984\n",
      "['[', 'Antidiabetic', 'Agents:', 'Recent', 'Advances', 'in', 'Their', 'Molecular', 'and', 'Clinical', 'Pharmacology', '[', 'ANTIDIABETIC', 'AGENTS:', 'RECENT', 'ADVANCES', 'IN', 'THEIR', 'MOLECULAR', 'AND', 'CLINICAL', 'PHARMACOLOGY', 'BY', 'Testa', '-.', 'Meyer,', '-.', 'Meyer', '(', 'Author', ')', 'May-28-1996[', 'ANTIDIABETIC', 'AGENTS:', 'RECENT', 'ADVANCES', 'IN', 'THEIR', 'MOLECULAR', 'AND', 'CLINICAL', 'PHARMACOLOGY', '[', 'ANTIDIABETIC', 'AGENTS:', 'RECENT', 'ADVANCES', 'IN', 'THEIR', 'MOLECULAR', 'AND', 'CLINICAL', 'PHARMACOLOGY', 'BY', 'TESTA', '-.', 'MEYER,', '-.', 'MEYER', '(', 'AUTHOR', ')', 'MAY-28-1996', ']', 'By', 'Testa', '-.', 'Meyer,', '-.', 'Meyer', '(', 'Author', ')May-28-1996', 'Hardcover']\n"
     ]
    }
   ],
   "source": [
    "#checking for the longest title -> we want to set padding to max_length instead of 512\n",
    "length_of_the_messages_test = test_set['TITLE'].str.split(\"\\\\s+\")\n",
    "length_of_the_messages_train = train_set['TITLE'].str.split(\"\\\\s+\")\n",
    "\n",
    "print(f\"Max number of words in test_set: {length_of_the_messages_test.str.len().max()}\")\n",
    "print(f\"Max number of words in train_set: {length_of_the_messages_train.str.len().max()}\")\n",
    "print(\"Index = \", length_of_the_messages_train.str.len().idxmax())\n",
    "\n",
    "print(length_of_the_messages_train[4984])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb8a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, path, train = False):\n",
    "\n",
    "        self.labels = [label for label in X['CATEGORY ID']]  #list\n",
    "        self.texts = [tokenizer(title, truncation=True, padding='max_length', max_length = 74,  #512\n",
    "                    return_tensors=\"pt\") for title in X['TITLE']]\n",
    "        self.images_name = [img_name for img_name in X['FILENAME']] \n",
    "        self.path = path\n",
    "        self.label_name = [lbl_name for lbl_name in X['CATEGORY']]\n",
    "        self.train = train\n",
    "        \n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def get_batch_label_name(self, idx):\n",
    "        return self.label_name[idx]\n",
    "            \n",
    "    def get_batch_images_names(self, idx):\n",
    "        # Fetch a batch of image names\n",
    "        return self.images_name[idx]\n",
    "    \n",
    "    def get_batch_images(self, idx):\n",
    "        # Fetch a batch of images\n",
    "        batch_images_name = self.get_batch_images_names(idx)\n",
    "        batch_label_name = self.get_batch_label_name(idx)\n",
    "        img = Image.open(os.path.join(self.path, batch_label_name, batch_images_name)).convert('RGB')\n",
    "        self.train = train\n",
    "        \n",
    "        if train:\n",
    "            train_transform  =  transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "            img = train_transform(img)\n",
    "        else:\n",
    "            val_transform  =  transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "            img = val_transform(img)\n",
    "                \n",
    "        return img\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        batch_image = self.get_batch_images(idx)\n",
    "        \n",
    "        return batch_texts, batch_y, batch_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44aab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45598.4 5699.8 5699.8\n",
      "LENGTH OF VALIDATION SET: 5701,       LENGTH OF TEST SET: 5699,        LENGTH OF TRAIN SET: 45598. \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased',\n",
    "                                         do_lower_case=True)\n",
    "length = len(train_set) + len(test_set)\n",
    "##print(len(test_set))\n",
    "#print(length)\n",
    "print(0.8*length, 0.1* length, 0.1*length)\n",
    "\n",
    "train_s=train_set[:45598] #change to train_set2\n",
    "val_s=train_set[45598:] #change to train_set2\n",
    "\n",
    "print(f'LENGTH OF VALIDATION SET: {len(val_s)}, \\\n",
    "      LENGTH OF TEST SET: {len(test_set)},  \\\n",
    "      LENGTH OF TRAIN SET: {len(train_s)}. ')\n",
    "\n",
    "train, val = Dataset(train_s, \"/home/gussikoju@GU.GU.SE/images/train\", True), Dataset(val_s, \"/home/gussikoju@GU.GU.SE/images/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246f3703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  101,  1103,  2377,  3955,   112,   188,  3043,  9506,   131,  1121,\n",
       "           1148,  3043,  1106,  2029, 10890,   113,  1103,  3043,  9506,  1116,\n",
       "           1326,   114,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]])},\n",
       " array(10),\n",
       " tensor([[[-1.7240, -1.7240, -1.7240,  ...,  0.1597, -0.6109, -1.2617],\n",
       "          [-1.7240, -1.7240, -1.7240,  ...,  1.7694,  1.2899,  0.2624],\n",
       "          [-1.7240, -1.7240, -1.7240,  ...,  2.0092,  2.0092,  1.8208],\n",
       "          ...,\n",
       "          [-0.6965, -0.6623, -0.8335,  ..., -0.1314, -0.0801,  0.0227],\n",
       "          [-0.6281, -0.5767, -0.8678,  ..., -0.9363, -0.8849, -0.8507],\n",
       "          [-0.6281, -0.6794, -0.8507,  ..., -1.2103, -1.1932, -1.1932]],\n",
       " \n",
       "         [[-0.7227, -0.7227, -0.7227,  ...,  0.8704,  0.0651, -0.5826],\n",
       "          [-0.7227, -0.7227, -0.7227,  ...,  2.3060,  1.8333,  0.8354],\n",
       "          [-0.7227, -0.7227, -0.7227,  ...,  2.3410,  2.3936,  2.2535],\n",
       "          ...,\n",
       "          [-0.5301, -0.4251, -0.5476,  ..., -0.1975, -0.1625, -0.0749],\n",
       "          [-0.3550, -0.2675, -0.4951,  ..., -0.9678, -0.8978, -0.8803],\n",
       "          [-0.2675, -0.3025, -0.4601,  ..., -1.0203, -1.0028, -0.9853]],\n",
       " \n",
       "         [[ 0.3742,  0.3742,  0.3742,  ...,  1.6117,  0.9145,  0.2696],\n",
       "          [ 0.3742,  0.3742,  0.3742,  ...,  2.6226,  2.3088,  1.4897],\n",
       "          [ 0.3742,  0.3742,  0.3742,  ...,  2.6051,  2.6400,  2.5529],\n",
       "          ...,\n",
       "          [-0.3927, -0.3230, -0.4798,  ..., -0.1661, -0.1312, -0.0441],\n",
       "          [-0.2707, -0.2010, -0.4624,  ..., -0.7064, -0.6541, -0.6367],\n",
       "          [-0.2010, -0.2532, -0.4275,  ..., -0.6367, -0.6193, -0.6193]]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "804309df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['linear.weight', 'linear.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#model_text = BertModel.from_pretrained('bert-base-cased', num_labels=30)\n",
    "\n",
    "#model_text.load_state_dict(torch.load('best_model.pth')['state_dict'], strict=False)\n",
    "#model_text.load_state_dict(torch.load('best_model.pth'),strict=False)\n",
    "\n",
    "model_state_dict = torch.load('best_model.pth')\n",
    "model_text = BertModel.from_pretrained( 'bert-base-cased', state_dict = model_state_dict, num_labels=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ebec49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2628263/3834321129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m    output_hidden_states = False)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#model_text.load_state_dict(torch.load('best_model.pth'),strict=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state_dict'"
     ]
    }
   ],
   "source": [
    "model_text = BertModel.from_pretrained('bert-base-cased', num_labels=30, output_attentions = False, \n",
    "   output_hidden_states = False)\n",
    "\n",
    "model_text.load_state_dict(torch.load('best_model.pth')['state_dict'], strict=False)\n",
    "#model_text.load_state_dict(torch.load('best_model.pth'),strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b35ccf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './images'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n",
    "                                             shuffle=True, num_workers=8)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.pretrained = torchvision.models.resnet50(pretrained=False)\n",
    "        self.pretrained.fc = torch.nn.Identity()\n",
    "        \n",
    "        self.linear = torch.nn.Linear(2048, 30)\n",
    "       \n",
    "\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.pretrained(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94a40cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_img = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_img.fc.in_features\n",
    "model_img.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2),\n",
    "                         nn.Linear(512, 30),\n",
    "                         nn.LogSoftmax(dim=1))\n",
    "#model_img = CNN()\n",
    "#model_img = model.cuda()\n",
    "model_img.load_state_dict(torch.load('CNN_model.pth'))\n",
    "\n",
    "#pretrained_model = torch.load('CNN_modelFTnew.pth')\n",
    "#model = models.resnet50(pretrained=False)\n",
    "#model.load_state_dict(pretrained_model.state_dict(),strict=False)\n",
    "#pretrained_model.load_state_dict(torch.load('CNN_modelFTnew.pth'),strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6076fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/53612835/size-mismatch-for-fc-bias-and-fc-weight-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05880ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, model_text, model_img):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.model_text = model_text\n",
    "        self.model_img = model_img\n",
    "        self.classifier = nn.Linear(30+30, 30)\n",
    "        \n",
    "    def forward(self, img, text, text_mask):\n",
    "        \n",
    "        input_text = self.model_text(text.view(74), text_mask.view(74))\n",
    "        input_img = self.model_img(img)\n",
    "       \n",
    "        input_all = torch.cat((input_text, input_img), dim=1)\n",
    "        \n",
    "        x = self.classifier(nn.functional.relu(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "my_model = MultimodalModel(model_text, model_img)\n",
    "#my_model = my_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c9e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "208ec4f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2784775/3423649551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print(textsss['attention_mask'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtekst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(labels.shape, labels.size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtekst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m74\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m74\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2784775/3349001452.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, text, text_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m74\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m74\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minput_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "text_new, labels, img= train[16]\n",
    "tekst, text_mask = text_new['attention_mask'], text_new['input_ids']\n",
    "#img = img.cuda()\n",
    "#text_new = text.cuda()\n",
    "#labels = labels.cuda()\n",
    "\n",
    "#print(textsss['attention_mask'])\n",
    "predictions = my_model(img.unsqueeze(0), tekst.unsqueeze(0), text_mask.unsqueeze(0))\n",
    "#print(labels.shape, labels.size)\n",
    "print(img.shape, tekst.view(74).shape, text_mask.view(74).shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c134d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(tekst.view(74).shape)\n",
    "print(tekst.view(74))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53002cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 74])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(tekst.shape)\n",
    "print(tekst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202da5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
